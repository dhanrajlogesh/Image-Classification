{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14053885-ae46-4639-ae62-239df05df97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhanraj/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dhanraj/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/dhanraj/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 44.7M/44.7M [00:00<00:00, 67.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.3243 - Train Acc: 0.8599 - Val Loss: 0.1805 - Val Acc: 0.9476\n",
      "Epoch 2/50 - Train Loss: 0.0645 - Train Acc: 0.9778 - Val Loss: 0.1823 - Val Acc: 0.9432\n",
      "Epoch 3/50 - Train Loss: 0.0287 - Train Acc: 0.9926 - Val Loss: 0.3913 - Val Acc: 0.8428\n",
      "Epoch 4/50 - Train Loss: 0.0121 - Train Acc: 0.9960 - Val Loss: 0.2444 - Val Acc: 0.9389\n",
      "Epoch 5/50 - Train Loss: 0.0170 - Train Acc: 0.9966 - Val Loss: 0.1802 - Val Acc: 0.9476\n",
      "Epoch 6/50 - Train Loss: 0.0080 - Train Acc: 0.9993 - Val Loss: 0.2341 - Val Acc: 0.9258\n",
      "Epoch 7/50 - Train Loss: 0.0066 - Train Acc: 1.0000 - Val Loss: 0.2384 - Val Acc: 0.9301\n",
      "Epoch 8/50 - Train Loss: 0.0038 - Train Acc: 1.0000 - Val Loss: 0.1986 - Val Acc: 0.9520\n",
      "Epoch 9/50 - Train Loss: 0.0029 - Train Acc: 1.0000 - Val Loss: 0.2217 - Val Acc: 0.9520\n",
      "Epoch 10/50 - Train Loss: 0.0019 - Train Acc: 1.0000 - Val Loss: 0.2117 - Val Acc: 0.9476\n",
      "Epoch 11/50 - Train Loss: 0.0018 - Train Acc: 1.0000 - Val Loss: 0.2086 - Val Acc: 0.9520\n",
      "Epoch 12/50 - Train Loss: 0.0029 - Train Acc: 0.9993 - Val Loss: 0.2220 - Val Acc: 0.9432\n",
      "Epoch 13/50 - Train Loss: 0.0101 - Train Acc: 0.9980 - Val Loss: 0.2295 - Val Acc: 0.9345\n",
      "Epoch 14/50 - Train Loss: 0.0106 - Train Acc: 0.9987 - Val Loss: 0.2283 - Val Acc: 0.9301\n",
      "Epoch 15/50 - Train Loss: 0.0027 - Train Acc: 1.0000 - Val Loss: 0.2312 - Val Acc: 0.9345\n",
      "Early stopping triggered.\n",
      "Best Validation Accuracy: 95.20%\n",
      "Classification Report on MidJourney Test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9554    0.9469    0.9511       113\n",
      "           1     0.9487    0.9569    0.9528       116\n",
      "\n",
      "    accuracy                         0.9520       229\n",
      "   macro avg     0.9520    0.9519    0.9520       229\n",
      "weighted avg     0.9520    0.9520    0.9520       229\n",
      "\n",
      "Test Accuracy: 95.20%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CIFAR_SAMPLES_PER_CLASS = 500  \n",
    "MIDJOURNEY_SAMPLES_PER_CLASS = 500  \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "PATIENCE = 7  \n",
    "\n",
    "MIDJOURNEY_BASE = '/home/dhanraj/Documents/Midjourney_Exp2'\n",
    "TRAIN_SPLIT = 'train'\n",
    "TEST_SPLIT = 'test'\n",
    "\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "])\n",
    "\n",
    "\n",
    "class MidJourneyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dir, split, transform=None, max_samples_per_class=500):\n",
    "        self.real_dir = os.path.join(base_dir, split, 'REAL')\n",
    "        self.fake_dir = os.path.join(base_dir, split, 'FAKE')\n",
    "        self.transform = transform\n",
    "\n",
    "        self.real_files = [os.path.join(self.real_dir, f) \n",
    "                           for f in os.listdir(self.real_dir) \n",
    "                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))][:max_samples_per_class]\n",
    "        self.fake_files = [os.path.join(self.fake_dir, f) \n",
    "                           for f in os.listdir(self.fake_dir) \n",
    "                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))][:max_samples_per_class]\n",
    "\n",
    "        self.file_list = self.real_files + self.fake_files\n",
    "        self.labels = [0]*len(self.real_files) + [1]*len(self.fake_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_list[idx]\n",
    "        img = Image.open(img_path).convert('RGB')        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "\n",
    "\n",
    "class CIFARRealSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, original_dataset, max_samples=500, transform=None):\n",
    "        self.transform = transform\n",
    "        self.max_samples = max_samples\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "\n",
    "        count = 0\n",
    "        for img, label in original_dataset:\n",
    "            if count >= max_samples:\n",
    "                break\n",
    "            self.data.append(img)\n",
    "            self.targets.append(0)\n",
    "            count += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data[idx]\n",
    "        label = self.targets[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def get_data_loaders():\n",
    "    cifar_train = datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "    cifar_train_subset = CIFARRealSubset(cifar_train, max_samples=CIFAR_SAMPLES_PER_CLASS, transform=train_transform)\n",
    "\n",
    "\n",
    "    mid_train = MidJourneyDataset(MIDJOURNEY_BASE, TRAIN_SPLIT, transform=train_transform, max_samples_per_class=MIDJOURNEY_SAMPLES_PER_CLASS)\n",
    "\n",
    "    train_dataset = torch.utils.data.ConcatDataset([cifar_train_subset, mid_train])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "    # MidJourney test set (real + fake) with test transform\n",
    "    mid_test = MidJourneyDataset(MIDJOURNEY_BASE, TEST_SPLIT, transform=test_transform, max_samples_per_class=500)\n",
    "    test_loader = DataLoader(mid_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2)  # Binary classification: real vs fake\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += inputs.size(0)\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += inputs.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc, np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "\n",
    "def main_train():\n",
    "    train_loader, test_loader = get_data_loaders()\n",
    "    model = get_model()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.3)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_acc, _, _ = evaluate(model, test_loader, criterion)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {train_loss:.4f} - Train Acc: {train_acc:.4f} - \"\n",
    "              f\"Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(f\"Best Validation Accuracy: {best_acc*100:.2f}%\")\n",
    "\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return model, test_loader\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trained_model, test_loader = main_train()\n",
    "\n",
    "    import sklearn.metrics as metrics\n",
    "\n",
    "    trained_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = trained_model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    print(\"Classification Report on MidJourney Test set:\")\n",
    "    print(metrics.classification_report(all_labels, all_preds, digits=4))\n",
    "    accuracy = metrics.accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53ec6d9-d4b3-4d9b-9294-d2d1343ef3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dhanraj/Downloads/lion.jpg => REAL\n",
      "/home/dhanraj/Downloads/srk.jpg => REAL\n",
      "/home/dhanraj/Downloads/person.jpeg => REAL\n",
      "/home/dhanraj/Downloads/mount.jpg => REAL\n",
      "/home/dhanraj/Downloads/girl.png => REAL\n",
      "/home/dhanraj/Downloads/person.jpeg => REAL\n",
      "/home/dhanraj/Downloads/IMG-20250811-WA0004.jpg => REAL\n",
      "/home/dhanraj/Downloads/IMG-20250811-WA0006.jpg => REAL\n",
      "/home/dhanraj/Downloads/kitty.jpeg => REAL\n",
      "/home/dhanraj/Downloads/ponnu.jpg => REAL\n",
      "/home/dhanraj/Downloads/Gina.png => FAKE\n",
      "/home/dhanraj/Downloads/girl.jpg => FAKE\n",
      "/home/dhanraj/Downloads/new.jpeg => REAL\n",
      "/home/dhanraj/Downloads/Tom.png => REAL\n",
      "/home/dhanraj/Downloads/Allan.png => REAL\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"best_model.pth\"\n",
    "\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "])\n",
    "\n",
    "\n",
    "def load_trained_model():\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(num_ftrs, 2)  # binary classes\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_single_image(image_path, model):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_t = test_transform(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_t)\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        pred_class = pred.item()\n",
    "\n",
    "    label_map = {0: \"REAL\", 1: \"FAKE\"}\n",
    "    return label_map[pred_class]\n",
    "\n",
    "def predict_multiple_images(image_paths, model):\n",
    "    results = []\n",
    "    for img_path in image_paths:\n",
    "        try:\n",
    "            pred_label = predict_single_image(img_path, model)\n",
    "            results.append((img_path, pred_label))\n",
    "        except Exception as e:\n",
    "            results.append((img_path, f\"Error: {e}\"))\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = load_trained_model()\n",
    "\n",
    "     test_image_list = [\n",
    "    '/home/dhanraj/Downloads/lion.jpg',\n",
    "    '/home/dhanraj/Downloads/srk.jpg',\n",
    "    '/home/dhanraj/Downloads/person.jpeg',\n",
    "    '/home/dhanraj/Downloads/mount.jpg',\n",
    "    '/home/dhanraj/Downloads/girl.png',\n",
    "    '/home/dhanraj/Downloads/person.jpeg',\n",
    "    '/home/dhanraj/Downloads/IMG-20250811-WA0004.jpg',\n",
    "    '/home/dhanraj/Downloads/IMG-20250811-WA0006.jpg',\n",
    "    '/home/dhanraj/Downloads/kitty.jpeg',\n",
    "    '/home/dhanraj/Downloads/ponnu.jpg',\n",
    "    '/home/dhanraj/Downloads/Gina.png',\n",
    "    '/home/dhanraj/Downloads/girl.jpg',\n",
    "    '/home/dhanraj/Downloads/new.jpeg',\n",
    "    '/home/dhanraj/Downloads/Tom.png',\n",
    "    '/home/dhanraj/Downloads/Allan.png',\n",
    "]\n",
    "\n",
    "\n",
    "predictions = predict_multiple_images(test_image_list, model)\n",
    "\n",
    "\n",
    "for path, pred in predictions:\n",
    "    print(f\"{path} => {pred}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73794591-2d87-42bf-9443-20e764f12c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
